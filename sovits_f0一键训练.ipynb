{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLSeed/DeepLearning/blob/main/sovits_f0%E4%B8%80%E9%94%AE%E8%AE%AD%E7%BB%83.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw61j0aOOoNZ"
      },
      "source": [
        "**[sovits合集导航](https://github.com/IceKyrin/sovits_guide)**\n",
        "\n",
        "**本colab为Rcell版sovits2.0(f0分支)，不支持单人模型，模型仅在三个一键脚本内部互通**\n",
        "\n",
        "可以使用**预模型（已替换为22050hz）**节省训练时间，id范围0-7，**最多八个人物**，按提示操作即可\n",
        "\n",
        "[sovits_f0](https://github.com/innnky/so-vits-svc)\n",
        "\n",
        "**默认每隔2000次step保存一次，可在“每隔多少次step保存一次断点”部分进行修改。**\n",
        "\n",
        "**在看到进度save之前不要轻易退出，以免丢失进度。**\n",
        "\n",
        "在此基础上修改为sovits配置，配合sovits一键制作数据集使用\n",
        "**为达最佳效果，建议下载公开歌声数据集opencpop，进行多人模型训练**\n",
        "\n",
        "**格式参考vits专栏三件套（评论区）**[vits注解](https://www.bilibili.com/read/cv18478187)\n",
        "\n",
        "95%的问题都可以参考专栏解决，剩下的我也不会了\n",
        "\n",
        "前置：[一键制作数据集](https://colab.research.google.com/drive/1avWZ_N5BsQcq45XkwQkDpmp912CLZS0n#scrollTo=xx2oAf90btEy)\n",
        "\n",
        "后置：[一键合成](https://colab.research.google.com/drive/1F3VpHCi9eridGw1F1hbqR7qhXGKuSCus#scrollTo=vjkgBV7j2cVJ)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看显卡\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "mZZr0oPu6vit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d70081b-14de-4269-cd5a-3c3da8bd59de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Oct 21 03:06:46 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "SaxypO5jwX__"
      },
      "outputs": [],
      "source": [
        "#@title 准备\n",
        "#@markdown 定义工具函数 `run_command` `run_command_by_line` `get_symbols` 和 `get_tensorboard_showing`\n",
        "# forked from https://www.endpointdev.com/blog/2015/01/getting-realtime-output-using-python/\n",
        "import os\n",
        "import subprocess\n",
        "def run_command(command_args):\n",
        "    def print_pipe(raw):\n",
        "        return print(raw.decode(\"utf-8\"), end='')\n",
        "    try:\n",
        "      process = subprocess.Popen(command_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "      out, err = process.communicate()\n",
        "    except:\n",
        "      pass\n",
        "    print_pipe(out)\n",
        "    print_pipe(err)\n",
        "    rc = process.poll()\n",
        "    return rc\n",
        "\n",
        "def run_command_by_line(command_args):\n",
        "    def print_pipe(raw):\n",
        "        return print(raw.decode(\"utf-8\"), end='')\n",
        "    with subprocess.Popen(command_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE) as process:\n",
        "      while process.poll() is None:\n",
        "        print_pipe(process.stdout.readline())\n",
        "      [print_pipe(line) for line in process.stderr.readlines()]\n",
        "    return\n",
        "\n",
        "def get_tensorboard_showing(logdir):\n",
        "    from multiprocessing import Process\n",
        "    from tensorboard import notebook\n",
        "    import tensorflow as tf\n",
        "    import time\n",
        "\n",
        "    def run_tb():\n",
        "        run_command_by_line([\"tensorboard\",\"--reload_interval\", \"30\",  \"--logdir\", logdir, \"--bind_all\"])\n",
        "    \n",
        "    def monitor_tb():\n",
        "        while True:\n",
        "            try:\n",
        "                notebook.display(height=998)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                time.sleep(3)\n",
        "\n",
        "    if param_enable_tb:\n",
        "        Process(target=run_tb).start()\n",
        "        Process(target=monitor_tb).start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i_0vZ-OjHVNu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8825b03a-a160-42f0-e86a-a852a61e7018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sovits_f0_train'...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Cython==0.29.21\n",
            "  Downloading Cython-0.29.21-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 13.9 MB/s \n",
            "\u001b[?25hCollecting librosa==0.8.0\n",
            "  Downloading librosa-0.8.0.tar.gz (183 kB)\n",
            "\u001b[K     |████████████████████████████████| 183 kB 71.1 MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.3.1\n",
            "  Downloading matplotlib-3.3.1-cp37-cp37m-manylinux1_x86_64.whl (11.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.6 MB 56.8 MB/s \n",
            "\u001b[?25hCollecting numpy==1.18.5\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting phonemizer==2.2.1\n",
            "  Downloading phonemizer-2.2.1-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting scipy==1.5.2\n",
            "  Downloading scipy-1.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting tensorboard==2.3.0\n",
            "  Downloading tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 48.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.13.1+cu113)\n",
            "Collecting Unidecode==1.1.1\n",
            "  Downloading Unidecode-1.1.1-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[K     |████████████████████████████████| 238 kB 63.7 MB/s \n",
            "\u001b[?25hCollecting pyworld\n",
            "  Downloading pyworld-0.3.1.tar.gz (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r requirements.txt (line 2)) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r requirements.txt (line 2)) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r requirements.txt (line 2)) (0.4.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r requirements.txt (line 2)) (0.56.3)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r requirements.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (2022.9.24)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.7/dist-packages (from phonemizer==2.2.1->-r requirements.txt (line 5)) (22.1.0)\n",
            "Collecting segments\n",
            "  Downloading segments-2.2.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (3.17.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (1.49.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.3.0->-r requirements.txt (line 7)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.3.0->-r requirements.txt (line 7)) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.3.0->-r requirements.txt (line 7)) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.3.0->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.3.1->-r requirements.txt (line 3)) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard==2.3.0->-r requirements.txt (line 7)) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.3.0->-r requirements.txt (line 7)) (3.9.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa==0.8.0->-r requirements.txt (line 2)) (0.39.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.0->-r requirements.txt (line 2)) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.0->-r requirements.txt (line 2)) (21.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.3.0->-r requirements.txt (line 7)) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.3.0->-r requirements.txt (line 7)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.3.0->-r requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.3.0->-r requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.3.0->-r requirements.txt (line 7)) (3.2.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.0->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa==0.8.0->-r requirements.txt (line 2)) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.8.0->-r requirements.txt (line 2)) (2.21)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segments->phonemizer==2.2.1->-r requirements.txt (line 5)) (2022.6.2)\n",
            "Collecting clldutils>=1.7.3\n",
            "  Downloading clldutils-3.12.0-py2.py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 67.5 MB/s \n",
            "\u001b[?25hCollecting csvw>=1.5.6\n",
            "  Downloading csvw-3.1.1-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from clldutils>=1.7.3->segments->phonemizer==2.2.1->-r requirements.txt (line 5)) (0.8.10)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from csvw>=1.5.6->segments->phonemizer==2.2.1->-r requirements.txt (line 5)) (4.3.3)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from csvw>=1.5.6->segments->phonemizer==2.2.1->-r requirements.txt (line 5)) (3.0.1)\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.2.0-py3-none-any.whl (500 kB)\n",
            "\u001b[K     |████████████████████████████████| 500 kB 70.2 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 659 kB/s \n",
            "\u001b[?25hCollecting rfc3986<2\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting language-tags\n",
            "  Downloading language_tags-1.1.0-py2.py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 76.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from csvw>=1.5.6->segments->phonemizer==2.2.1->-r requirements.txt (line 5)) (2.10.3)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel->csvw>=1.5.6->segments->phonemizer==2.2.1->-r requirements.txt (line 5)) (2022.4)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer==2.2.1->-r requirements.txt (line 5)) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer==2.2.1->-r requirements.txt (line 5)) (5.10.0)\n",
            "Building wheels for collected packages: librosa, pyworld\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.8.0-py3-none-any.whl size=201396 sha256=b3ce66885fa1b90c7cd8ccf5021a9c1419a4b83e111674e07215a2a714344a40\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/1e/aa/d91797ae7e1ce11853ee100bee9d1781ae9d750e7458c95afb\n",
            "  Building wheel for pyworld (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyworld: filename=pyworld-0.3.1-cp37-cp37m-linux_x86_64.whl size=611976 sha256=71bbf03d9ecd374daa91d20d41bba2fe8f37f0f6c8f80a2d591c8ecd02e5b5fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/23/da/9c5f21eaa1dedc0434d00411ec949b3cb678f8ac3b07a1764b\n",
            "Successfully built librosa pyworld\n",
            "Installing collected packages: isodate, rfc3986, rdflib, language-tags, colorama, numpy, csvw, colorlog, scipy, clldutils, segments, Cython, Unidecode, tensorboard, pyworld, phonemizer, matplotlib, librosa\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 0.29.32\n",
            "    Uninstalling Cython-0.29.32:\n",
            "      Successfully uninstalled Cython-0.29.32\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.8.1\n",
            "    Uninstalling librosa-0.8.1:\n",
            "      Successfully uninstalled librosa-0.8.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n",
            "tensorflow 2.9.2 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "tensorflow 2.9.2 requires tensorboard<2.10,>=2.9, but you have tensorboard 2.3.0 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "jaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.23 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "cmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "Successfully installed Cython-0.29.21 Unidecode-1.1.1 clldutils-3.12.0 colorama-0.4.5 colorlog-6.7.0 csvw-3.1.1 isodate-0.6.1 language-tags-1.1.0 librosa-0.8.0 matplotlib-3.3.1 numpy-1.18.5 phonemizer-2.2.1 pyworld-0.3.1 rdflib-6.2.0 rfc3986-1.5.0 scipy-1.5.2 segments-2.2.1 tensorboard-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  espeak-data libespeak1 libportaudio2 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak espeak-data libespeak1 libportaudio2 libsonic0\n",
            "0 upgraded, 5 newly installed, 0 to remove and 27 not upgraded.\n",
            "Need to get 1,219 kB of archives.\n",
            "After this operation, 3,031 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsonic0 amd64 0.2.0-6 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak-data amd64 1.48.04+dfsg-5 [934 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libespeak1 amd64 1.48.04+dfsg-5 [145 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak amd64 1.48.04+dfsg-5 [61.6 kB]\n",
            "Fetched 1,219 kB in 1s (1,452 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 123942 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../libsonic0_0.2.0-6_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-6) ...\n",
            "Selecting previously unselected package espeak-data:amd64.\n",
            "Preparing to unpack .../espeak-data_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking espeak-data:amd64 (1.48.04+dfsg-5) ...\n",
            "Selecting previously unselected package libespeak1:amd64.\n",
            "Preparing to unpack .../libespeak1_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking libespeak1:amd64 (1.48.04+dfsg-5) ...\n",
            "Selecting previously unselected package espeak.\n",
            "Preparing to unpack .../espeak_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking espeak (1.48.04+dfsg-5) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1) ...\n",
            "Setting up espeak-data:amd64 (1.48.04+dfsg-5) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-6) ...\n",
            "Setting up libespeak1:amd64 (1.48.04+dfsg-5) ...\n",
            "Setting up espeak (1.48.04+dfsg-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "p7zip-full is already the newest version (16.02+dfsg-6).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  p7zip-rar\n",
            "0 upgraded, 1 newly installed, 0 to remove and 27 not upgraded.\n",
            "Need to get 43.1 kB of archives.\n",
            "After this operation, 113 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 p7zip-rar amd64 16.02-2 [43.1 kB]\n",
            "Fetched 43.1 kB in 0s (143 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package p7zip-rar.\n",
            "(Reading database ... 124267 files and directories currently installed.)\n",
            "Preparing to unpack .../p7zip-rar_16.02-2_amd64.deb ...\n",
            "Unpacking p7zip-rar (16.02-2) ...\n",
            "Setting up p7zip-rar (16.02-2) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting demjson\n",
            "  Downloading demjson-2.2.4.tar.gz (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 14.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: demjson\n",
            "  Building wheel for demjson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for demjson: filename=demjson-2.2.4-py3-none-any.whl size=73565 sha256=7d28e747cf9107a43484e86c47b4aec3183954fa7fc2e8b663f06a5c4b9366f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/94/3d/466801f4a8db8e6fce765d7a0115dfebcc55ddf6b00cd98f59\n",
            "Successfully built demjson\n",
            "Installing collected packages: demjson\n",
            "Successfully installed demjson-2.2.4\n"
          ]
        }
      ],
      "source": [
        "#@title 下载依赖库\n",
        "#@markdown 取消勾选则不会节省空间\n",
        "colab_save_space = True #@param {type:\"boolean\"}\n",
        "os.chdir('/content')\n",
        "run_command_by_line([\"git\", \"clone\", \"https://github.com/DLSeed/sovits_f0_train.git\", \"-b\", \"main\" if colab_save_space else \"main\"])\n",
        "os.chdir('/content/sovits_f0_train')\n",
        "!pip install -r requirements.txt\n",
        "!sudo apt-get install espeak -y\n",
        "!sudo apt-get install p7zip-full p7zip-rar\n",
        "!pip install demjson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZOgjdsQgKTfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08e84ff1-30a9-479f-916d-6f60d0ad061f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title 加载Google云端硬盘\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "N3a-FsHghwXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9f83a73-1199-4849-d410-f2a9bd1a94c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sovits_f0_train/dataset\n"
          ]
        }
      ],
      "source": [
        "#@title 解压数据集\n",
        "!mkdir /content/sovits_f0_train/dataset\n",
        "\n",
        "#@markdown **数据集一键制作出的压缩包，名称完全相同**\n",
        "\n",
        "#@markdown 每次解压**一个**，然后确认完成后**修改名称**、**依次**解压**所有**压缩包\n",
        "\n",
        "#@markdown 压缩包名称(不带zip)\n",
        "DATASETNAME = \"out_lamianer\"  #@param {type:\"string\"}\n",
        "#@markdown 压缩包路径(这个名字是接着上一篇的输出zip)\n",
        "ZIP_PATH = \"/content/drive/MyDrive/dataset/out_lamianer.zip\"  #@param {type:\"string\"}\n",
        "DATASETPATH = \"/content/sovits_f0_train/\" + DATASETNAME\n",
        "%cd /content/sovits_f0_train/dataset\n",
        "!cp {ZIP_PATH} {DATASETNAME}.zip\n",
        "!unzip -q {DATASETNAME}.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 生成filelist\n",
        "import re\n",
        "import demjson\n",
        "!mkdir /content/sovits_f0_train/filelist\n",
        "%cd /content/sovits_f0_train/\n",
        "\n",
        "\n",
        "def replace_id(f_list, sp_id):\n",
        "    if re.search('wav[|](\\d+)[|]', f_list[0]):\n",
        "        split_char = \"wav|%s|\" % re.search('wav[|](\\d+)[|]', f_list[0]).group(1)\n",
        "    else:\n",
        "        split_char = \"wav|\"\n",
        "    f_list = [x.replace(split_char, f\"wav|{sp_id}|\") for x in f_list]\n",
        "    return f_list\n",
        "\n",
        "\n",
        "# data文件夹下放，各个 工程名/wavs/train(val)(这层可以不要)/xxx.wav\n",
        "# 自动读取每个工程文件夹下的train(val).txt(xxx.wav|无符号文本)\n",
        "#@markdown 默认为0\n",
        "pre = 0  #@param {type:\"string\"} \n",
        "dataset_path = \"./dataset\"  # 数据合集目录\n",
        "file_train_name = \"train.txt\"\n",
        "file_val_name = \"val.txt\"\n",
        "#@markdown 人物名(**必须与数据集制作时人物名相同，个数也相同，英文逗号隔开**)可以单人\n",
        "\n",
        "#@markdown 就是已经在dataset文件夹下解压出的数据文件夹\n",
        "\n",
        "#@markdown id即为此处人物排序，从0自动开始计算\n",
        "speakers = \"lamianer\" #@param {type:\"string\"}\n",
        "speakers = [speaker.strip() for speaker in speakers.split(\",\")]\n",
        "\n",
        "print(speakers)\n",
        "\n",
        "train_f = open(\"./filelist/train.txt\", \"w\", encoding=\"utf-8\")\n",
        "val_f = open(\"./filelist/val.txt\", \"w\", encoding=\"utf-8\")\n",
        "pre = int(pre)\n",
        "for i in range(pre, len(speakers) + pre):\n",
        "    with open(f\"{dataset_path}/{speakers[i]}/{file_train_name}\", \"r\", encoding=\"utf-8\") as f:\n",
        "        file_list = f.readlines()\n",
        "        train_f.writelines(replace_id(file_list, i))\n",
        "    with open(f\"{dataset_path}/{speakers[i]}/{file_val_name}\", \"r\", encoding=\"utf-8\") as f:\n",
        "        file_list = f.readlines()\n",
        "        val_f.writelines(replace_id(file_list, i))\n",
        "train_f.close()\n",
        "val_f.close()\n"
      ],
      "metadata": {
        "id": "GfkmTW1Z36iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e3d57c3-5546-4bd3-bcb5-1a8fdb0de3e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sovits_f0_train\n",
            "['lamianer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qDPyW6pnMpK9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b0P9X9SSPl0L",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2cec917-500d-4c04-f5bb-ba2ac05e0fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speakers: \n",
            "\t0: lamianer\n"
          ]
        }
      ],
      "source": [
        "#@title 生成配置文件 在configs下，记得自己下载保留一份，下次用\n",
        "#@markdown 本colab内可选下载的**预模型已替换为22050hz**，id为0-7可用\n",
        "\n",
        "#@markdown **预模型请直接使用下一步下载的nyarumul.json，id0-7可用，json无需修改**\n",
        "\n",
        "#@markdown **预模型的json可以不改动，只放一个id的数据进去**\n",
        "\n",
        "# forked from https://github.com/CjangCjengh/vits/blob/main/configs/japanese_ss_base2.json\n",
        "\n",
        "#@markdown 44100采样率，训练时长要求高\n",
        "high_sample_rate = False #@param {type:\"boolean\"}\n",
        "#@markdown 配置文件名称\n",
        "json_filename = \"lamianer.json\" #@param {type:\"string\"}\n",
        "#@markdown 训练次数\n",
        "hparams_epochs = 6000 #@param {type:\"integer\"}\n",
        "#@markdown 每隔多少次step保存一次断点\n",
        "hparams_eval_interval = 2000 #@param {type:\"integer\"}\n",
        "#@markdown 单次step的文件数（建议在16以内，若使用44100采样率，在8以内）\n",
        "hparams_batch_size = 16 #@param {type:\"integer\"}\n",
        "#@markdown 训练集文件列表\n",
        "hparams_training_files = \"/content/sovits_f0_train/filelist/train.txt\" #@param {type:\"string\"}\n",
        "#@markdown 验证集文件列表\n",
        "hparams_validation_files = \"/content/sovits_f0_train/filelist/val.txt\"#@param {type:\"string\"}\n",
        "#@markdown 人物名，多个人物用英文逗号隔开，必须与上面填写的相同\n",
        "hparams_speaker = \"lamianer\" #@param {type:\"string\"}\n",
        "#@markdown 模型名\n",
        "hparams_model_name = \"nyarumul\" #@param {type:\"string\"}\n",
        "\n",
        "speakers = [speaker.strip() for speaker in hparams_speaker.split(\",\")]\n",
        "print(\"speakers: \")\n",
        "for i, speaker in enumerate(speakers):\n",
        "  print(\"\\t{a}: {b}\".format(a=i, b=speaker))\n",
        "training_json = {\n",
        "  \"train\": {\n",
        "    \"log_interval\": 200,\n",
        "    \"eval_interval\": hparams_eval_interval,\n",
        "    \"seed\": 1234 ,\n",
        "    \"epochs\": hparams_epochs,\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"betas\": [0.8, 0.99],\n",
        "    \"eps\": 1e-9,\n",
        "    \"batch_size\": hparams_batch_size,\n",
        "    \"fp16_run\": True,\n",
        "    \"lr_decay\": 0.999875,\n",
        "    \"segment_size\": 16384 if high_sample_rate else 8192,\n",
        "    \"init_lr_ratio\": 1,\n",
        "    \"warmup_epochs\": 0,\n",
        "    \"c_mel\": 45,\n",
        "    \"c_kl\": 1.0\n",
        "  },\n",
        "  \"data\": {\n",
        "    \"training_files\": hparams_training_files,\n",
        "    \"validation_files\": hparams_validation_files,\n",
        "    \"text_cleaners\":[\"english_cleaners2\"],\n",
        "    \"max_wav_value\": 32768.0,\n",
        "    \"sampling_rate\": 44100 if high_sample_rate else 22050,\n",
        "    \"filter_length\": 2048 if high_sample_rate else 1024,\n",
        "    \"hop_length\": 512 if high_sample_rate else 256,\n",
        "    \"win_length\": 2048 if high_sample_rate else 1024,\n",
        "    \"n_mel_channels\": 128 if high_sample_rate else 80,\n",
        "    \"mel_fmin\": 0.0,\n",
        "    \"mel_fmax\": None,\n",
        "    \"add_blank\": True,\n",
        "    \"n_speakers\": len(speakers) if len(speakers) > 1 else 2\n",
        "  },\n",
        "  \"model\": {\n",
        "    \"inter_channels\": 192,\n",
        "    \"hidden_channels\": 256,\n",
        "    \"filter_channels\": 768,\n",
        "    \"n_heads\": 2,\n",
        "    \"n_layers\": 6,\n",
        "    \"kernel_size\": 3,\n",
        "    \"p_dropout\": 0.1,\n",
        "    \"resblock\": \"1\",\n",
        "    \"resblock_kernel_sizes\": [3,7,11],\n",
        "    \"resblock_dilation_sizes\": [[1,3,5], [1,3,5], [1,3,5]],\n",
        "    \"upsample_rates\": [8,8,4,2] if high_sample_rate else [8,8,2,2],\n",
        "    \"upsample_initial_channel\": 512,\n",
        "    \"upsample_kernel_sizes\": [16,16,4,4],\n",
        "    \"n_layers_q\": 3,\n",
        "    \"use_spectral_norm\": False\n",
        "  },\n",
        "  \"speakers\": speakers\n",
        "}\n",
        "\n",
        "if len(speakers) > 1:\n",
        "  training_json[\"model\"][\"gin_channels\"] = 256\n",
        "\n",
        "import demjson\n",
        "os.chdir('/content/sovits_f0_train/configs')\n",
        "training_json_text = demjson.encode(training_json)\n",
        "with open(json_filename, \"w\") as file:\n",
        "  file.write(training_json_text)\n",
        "\n",
        "os.chdir('/content/sovits_f0_train')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oqSbYJ5ack09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d53b39e-967f-49d1-f34f-378f06816ac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling core.pyx because it changed.\n",
            "[1/1] Cythonizing core.pyx\n",
            "/usr/local/lib/python3.7/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/sovits_f0_train/monotonic_align/core.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "running build_ext\n",
            "building 'monotonic_align.core' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/include/python3.7m -c core.c -o build/temp.linux-x86_64-3.7/core.o\n",
            "creating /content/sovits_f0_train/monotonic_align/monotonic_align\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/core.o -o /content/sovits_f0_train/monotonic_align/monotonic_align/core.cpython-37m-x86_64-linux-gnu.so\n"
          ]
        }
      ],
      "source": [
        "#@title 预处理\n",
        "os.chdir('/content/sovits_f0_train/monotonic_align')\n",
        "!python setup.py build_ext --inplace\n",
        "os.chdir('/content/sovits_f0_train')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "若**使用预模型**（2.0只能用2.0的预模型），直接在预模型基础上训练，节省时间\n",
        "\n",
        "使用预模型读取pth文件后，文件名的G_steps.pth，steps会乱是正常现象（使用v100训练，batch_size为32，colab显存不够改成了16，所以会乱掉，保存最新生成的模型即可）\n",
        "\n",
        "程序读取一次预模型后，以后断点恢复训练、用**最新生成的一组pth继续训练**"
      ],
      "metadata": {
        "id": "mXsu-rXYx2Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 22050hz预模型,id为0-7可用\n",
        "!mkdir /content/drive/MyDrive/nyarumul/\n",
        "pre_pth = True #@param {type:\"boolean\"}\n",
        "if pre_pth:\n",
        "  !wget https://huggingface.co/spaces/xiaolang/sovits_f0/resolve/main/G_50000.pth -O /content/drive/MyDrive/nyarumul/G_50000.pth\n",
        "  !wget https://huggingface.co/spaces/xiaolang/sovits_f0/resolve/main/D_50000.pth -O /content/drive/MyDrive/nyarumul/D_50000.pth"
      ],
      "metadata": {
        "id": "nz3KJCcBVTlX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ad2c88-0577-4c08-c0a8-ee5f33ea4cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-09 00:53:02--  https://huggingface.co/spaces/xiaolang/sovits_f0/resolve/main/G_50000.pth\n",
            "Resolving huggingface.co (huggingface.co)... 54.210.225.113, 54.173.5.192, 52.5.54.249, ...\n",
            "Connecting to huggingface.co (huggingface.co)|54.210.225.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/89/cb/89cb9b81ebb299ff59bb93f7dd9c2ff81373001bb318b1e3d4df28ab738185bd/531507463f55c4469b4a8f603f6cc61d44a4c060e3dd89b0c4618fffc4b79441?response-content-disposition=attachment%3B%20filename%3D%22G_50000.pth%22&Expires=1665535982&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzg5L2NiLzg5Y2I5YjgxZWJiMjk5ZmY1OWJiOTNmN2RkOWMyZmY4MTM3MzAwMWJiMzE4YjFlM2Q0ZGYyOGFiNzM4MTg1YmQvNTMxNTA3NDYzZjU1YzQ0NjliNGE4ZjYwM2Y2Y2M2MWQ0NGE0YzA2MGUzZGQ4OWIwYzQ2MThmZmZjNGI3OTQ0MT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMkdfNTAwMDAucHRoJTIyIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjY1NTM1OTgyfX19XX0_&Signature=a4yUv7-gb5H3mPaZEUkHhzwxx6LugWQ9I3JUAb7r8f32oowBLf03NrSDv-cE2Quh4Y1jDi1z7UvKDY84C~6NBrUUAipSecgA15RG--MbbY03G3jnGgpaKcU1DzXcC9oxwDChRGTKiRDAmNA8NtGBXKravEgSuPFmhuJhr0OKuReTI4lqaopYxBm-~-OzpuSBRNuPQxcc167X-UT98K3WV3P1trPtYq50GS-jxayKHZ3gUNuAnkkYVuuq-HgDBqsDrWMOaqpEnCp8aCrptJ8s444W3BUnyoy-rhJwkZDeVKCzLtycNVF0tNH7By3UmDcvwqo9jXE1ZUMv8BGk2eR1hQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2022-10-09 00:53:02--  https://cdn-lfs.huggingface.co/repos/89/cb/89cb9b81ebb299ff59bb93f7dd9c2ff81373001bb318b1e3d4df28ab738185bd/531507463f55c4469b4a8f603f6cc61d44a4c060e3dd89b0c4618fffc4b79441?response-content-disposition=attachment%3B%20filename%3D%22G_50000.pth%22&Expires=1665535982&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzg5L2NiLzg5Y2I5YjgxZWJiMjk5ZmY1OWJiOTNmN2RkOWMyZmY4MTM3MzAwMWJiMzE4YjFlM2Q0ZGYyOGFiNzM4MTg1YmQvNTMxNTA3NDYzZjU1YzQ0NjliNGE4ZjYwM2Y2Y2M2MWQ0NGE0YzA2MGUzZGQ4OWIwYzQ2MThmZmZjNGI3OTQ0MT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMkdfNTAwMDAucHRoJTIyIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjY1NTM1OTgyfX19XX0_&Signature=a4yUv7-gb5H3mPaZEUkHhzwxx6LugWQ9I3JUAb7r8f32oowBLf03NrSDv-cE2Quh4Y1jDi1z7UvKDY84C~6NBrUUAipSecgA15RG--MbbY03G3jnGgpaKcU1DzXcC9oxwDChRGTKiRDAmNA8NtGBXKravEgSuPFmhuJhr0OKuReTI4lqaopYxBm-~-OzpuSBRNuPQxcc167X-UT98K3WV3P1trPtYq50GS-jxayKHZ3gUNuAnkkYVuuq-HgDBqsDrWMOaqpEnCp8aCrptJ8s444W3BUnyoy-rhJwkZDeVKCzLtycNVF0tNH7By3UmDcvwqo9jXE1ZUMv8BGk2eR1hQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.64.49, 108.138.64.121, 108.138.64.36, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.64.49|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 663876145 (633M) [application/zip]\n",
            "Saving to: ‘/content/drive/MyDrive/nyarumul/G_50000.pth’\n",
            "\n",
            "/content/drive/MyDr 100%[===================>] 633.12M  61.8MB/s    in 10s     \n",
            "\n",
            "2022-10-09 00:53:12 (62.4 MB/s) - ‘/content/drive/MyDrive/nyarumul/G_50000.pth’ saved [663876145/663876145]\n",
            "\n",
            "--2022-10-09 00:53:12--  https://huggingface.co/spaces/xiaolang/sovits_f0/resolve/main/D_50000.pth\n",
            "Resolving huggingface.co (huggingface.co)... 54.210.225.113, 54.173.5.192, 52.5.54.249, ...\n",
            "Connecting to huggingface.co (huggingface.co)|54.210.225.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/89/cb/89cb9b81ebb299ff59bb93f7dd9c2ff81373001bb318b1e3d4df28ab738185bd/0a1c58dc90076481461e7bbef5b632ac8844f9176b299dbf6b9e8ecfb9034638?response-content-disposition=attachment%3B%20filename%3D%22D_50000.pth%22&Expires=1665503673&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzg5L2NiLzg5Y2I5YjgxZWJiMjk5ZmY1OWJiOTNmN2RkOWMyZmY4MTM3MzAwMWJiMzE4YjFlM2Q0ZGYyOGFiNzM4MTg1YmQvMGExYzU4ZGM5MDA3NjQ4MTQ2MWU3YmJlZjViNjMyYWM4ODQ0ZjkxNzZiMjk5ZGJmNmI5ZThlY2ZiOTAzNDYzOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMkRfNTAwMDAucHRoJTIyIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjY1NTAzNjczfX19XX0_&Signature=ZvtLskjZ~yIRIUQiFtENQPCAtS21FHq5qBZuK78CijFQIGzBaHHrKLEH9SFwSeD~79aLBIjYodoeZjX4bie0Qn5RMWbu9gGMZT20TrzoERJ2fW6oz7wVhydVLM6gi1aAx~O~VrYNUNHv-ji8wayCjq3VRpMLIxB7Fg9GwPjw45vAnP~mWSxwx8vnw9J3ZLb8wZhEsYp5a3OMzFbvOm10~ihprWR0O~MKs5vabfH7kevfSGNy3-XDsSp4yZFjEpz6ILehuoKmEVCILuHqWuHTRNivHY9Am~O8yUYNGHTOptYdhGomeIqTwbvfSLCSRRcBgt184IbX8NK8B6CvF-CjHw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2022-10-09 00:53:12--  https://cdn-lfs.huggingface.co/repos/89/cb/89cb9b81ebb299ff59bb93f7dd9c2ff81373001bb318b1e3d4df28ab738185bd/0a1c58dc90076481461e7bbef5b632ac8844f9176b299dbf6b9e8ecfb9034638?response-content-disposition=attachment%3B%20filename%3D%22D_50000.pth%22&Expires=1665503673&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzg5L2NiLzg5Y2I5YjgxZWJiMjk5ZmY1OWJiOTNmN2RkOWMyZmY4MTM3MzAwMWJiMzE4YjFlM2Q0ZGYyOGFiNzM4MTg1YmQvMGExYzU4ZGM5MDA3NjQ4MTQ2MWU3YmJlZjViNjMyYWM4ODQ0ZjkxNzZiMjk5ZGJmNmI5ZThlY2ZiOTAzNDYzOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMkRfNTAwMDAucHRoJTIyIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjY1NTAzNjczfX19XX0_&Signature=ZvtLskjZ~yIRIUQiFtENQPCAtS21FHq5qBZuK78CijFQIGzBaHHrKLEH9SFwSeD~79aLBIjYodoeZjX4bie0Qn5RMWbu9gGMZT20TrzoERJ2fW6oz7wVhydVLM6gi1aAx~O~VrYNUNHv-ji8wayCjq3VRpMLIxB7Fg9GwPjw45vAnP~mWSxwx8vnw9J3ZLb8wZhEsYp5a3OMzFbvOm10~ihprWR0O~MKs5vabfH7kevfSGNy3-XDsSp4yZFjEpz6ILehuoKmEVCILuHqWuHTRNivHY9Am~O8yUYNGHTOptYdhGomeIqTwbvfSLCSRRcBgt184IbX8NK8B6CvF-CjHw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.64.49, 108.138.64.121, 108.138.64.36, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.64.49|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 561098185 (535M) [application/zip]\n",
            "Saving to: ‘/content/drive/MyDrive/nyarumul/D_50000.pth’\n",
            "\n",
            "/content/drive/MyDr 100%[===================>] 535.10M  49.8MB/s    in 11s     \n",
            "\n",
            "2022-10-09 00:53:23 (49.4 MB/s) - ‘/content/drive/MyDrive/nyarumul/D_50000.pth’ saved [561098185/561098185]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltU2JXpxIh-K"
      },
      "outputs": [],
      "source": [
        "#@title 训练\n",
        "\n",
        "#@markdown 启用tensorboard可视化数据，首次运行可能十分钟左右出现数据\n",
        "\n",
        "#@markdown epoch次数看/content/drive/MyDrive/数据集名/train.log\n",
        "\n",
        "#@markdown 数据集名（使用预模型则为nyarumul）\n",
        "hparams_model_name = \"nyarumul\"  # @param {type:\"string\"}\n",
        "#@markdown 模型自动保存到 /云盘路径/模型名 ,记得定期清理空间！\n",
        "param_enable_tb = True  # @param {type:\"boolean\"}\n",
        "if param_enable_tb:\n",
        "  #@markdown 云盘路径（一般不改） （定时清云盘！！！pth里保留最新的俩）\n",
        "  logdir = \"/content/drive/MyDrive/\"  # @param {type:\"string\"}\n",
        "  new_pth_dir = os.path.join(logdir, hparams_model_name)\n",
        "  get_tensorboard_showing(new_pth_dir)\n",
        "os.chdir('/content/sovits_f0_train')\n",
        "#@markdown 配置文件json名（使用预模型则为nyarumul.json）\n",
        "json_filename = \"nyarumul.json\"  # @param {type:\"string\"}\n",
        "# 这里魔改过until.py的args\n",
        "run_command_by_line([\"python\", \"train_ms.py\", \"-c\", \"configs/{json}\".format(json=json_filename), \"-m\", hparams_model_name, \"-l\",logdir])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 精简模型\n",
        "import torch\n",
        "#@markdown 模型名，数字是最新的模型steps\n",
        "new_model_name = \"G_16000.pth\"  # @param {type:\"string\"}\n",
        "checkpoint_dict = torch.load(f\"{new_pth_dir}/{new_model_name}\")\n",
        "iteration = checkpoint_dict['iteration']\n",
        "learning_rate = checkpoint_dict['learning_rate']\n",
        "optimizer = checkpoint_dict['optimizer']\n",
        "saved_state_dict = checkpoint_dict['model']\n",
        "print(iteration)\n",
        "#@markdown 输出xxx_epoch.pth\n",
        "torch.save({'model': saved_state_dict,\n",
        "  'iteration': None,\n",
        "  'optimizer': None,\n",
        "  'learning_rate': None}, f'{new_pth_dir}/{iteration}_epochs.pth')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lwa9GQx1X0L_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01678459-66d9-45cb-eb79-00fe53ec93bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "321\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}