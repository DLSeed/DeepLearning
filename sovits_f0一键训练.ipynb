{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLSeed/DeepLearning/blob/main/sovits_f0%E4%B8%80%E9%94%AE%E8%AE%AD%E7%BB%83.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw61j0aOOoNZ"
      },
      "source": [
        "**[sovits合集导航](https://github.com/IceKyrin/sovits_guide)**\n",
        "\n",
        "**本colab为Rcell版sovits2.0(f0分支)，不支持单人模型，模型仅在三个一键脚本内部互通**\n",
        "\n",
        "可以使用**预模型（已替换为22050hz）**节省训练时间，id范围0-7，**最多八个人物**，按提示操作即可\n",
        "\n",
        "[sovits_f0](https://github.com/innnky/so-vits-svc)\n",
        "\n",
        "**默认每隔2000次step保存一次，可在“每隔多少次step保存一次断点”部分进行修改。**\n",
        "\n",
        "**在看到进度save之前不要轻易退出，以免丢失进度。**\n",
        "\n",
        "在此基础上修改为sovits配置，配合sovits一键制作数据集使用\n",
        "**为达最佳效果，建议下载公开歌声数据集opencpop，进行多人模型训练**\n",
        "\n",
        "**格式参考vits专栏三件套（评论区）**[vits注解](https://www.bilibili.com/read/cv18478187)\n",
        "\n",
        "95%的问题都可以参考专栏解决，剩下的我也不会了\n",
        "\n",
        "前置：[一键制作数据集](https://colab.research.google.com/drive/1avWZ_N5BsQcq45XkwQkDpmp912CLZS0n#scrollTo=xx2oAf90btEy)\n",
        "\n",
        "后置：[一键合成](https://colab.research.google.com/drive/1F3VpHCi9eridGw1F1hbqR7qhXGKuSCus#scrollTo=vjkgBV7j2cVJ)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看显卡\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "mZZr0oPu6vit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7d7f4a1-dba8-45cc-86fc-8f1a8f07877c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov  5 14:24:00 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "SaxypO5jwX__"
      },
      "outputs": [],
      "source": [
        "#@title 准备\n",
        "#@markdown 定义工具函数 `run_command` `run_command_by_line` `get_symbols` 和 `get_tensorboard_showing`\n",
        "# forked from https://www.endpointdev.com/blog/2015/01/getting-realtime-output-using-python/\n",
        "import os\n",
        "import subprocess\n",
        "def run_command(command_args):\n",
        "    def print_pipe(raw):\n",
        "        return print(raw.decode(\"utf-8\"), end='')\n",
        "    try:\n",
        "      process = subprocess.Popen(command_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "      out, err = process.communicate()\n",
        "    except:\n",
        "      pass\n",
        "    print_pipe(out)\n",
        "    print_pipe(err)\n",
        "    rc = process.poll()\n",
        "    return rc\n",
        "\n",
        "def run_command_by_line(command_args):\n",
        "    def print_pipe(raw):\n",
        "        return print(raw.decode(\"utf-8\"), end='')\n",
        "    with subprocess.Popen(command_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE) as process:\n",
        "      while process.poll() is None:\n",
        "        print_pipe(process.stdout.readline())\n",
        "      [print_pipe(line) for line in process.stderr.readlines()]\n",
        "    return\n",
        "\n",
        "def get_tensorboard_showing(logdir):\n",
        "    from multiprocessing import Process\n",
        "    from tensorboard import notebook\n",
        "    import tensorflow as tf\n",
        "    import time\n",
        "\n",
        "    def run_tb():\n",
        "        run_command_by_line([\"tensorboard\",\"--reload_interval\", \"30\",  \"--logdir\", logdir, \"--bind_all\"])\n",
        "    \n",
        "    def monitor_tb():\n",
        "        while True:\n",
        "            try:\n",
        "                notebook.display(height=998)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                time.sleep(3)\n",
        "\n",
        "    if param_enable_tb:\n",
        "        Process(target=run_tb).start()\n",
        "        Process(target=monitor_tb).start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i_0vZ-OjHVNu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c95c8de-9e3f-4f8a-a598-19350f929285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sovits_f0_train'...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Cython==0.29.21\n",
            "  Downloading Cython-0.29.21-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting librosa==0.8.0\n",
            "  Downloading librosa-0.8.0.tar.gz (183 kB)\n",
            "\u001b[K     |████████████████████████████████| 183 kB 61.6 MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.3.1\n",
            "  Downloading matplotlib-3.3.1-cp37-cp37m-manylinux1_x86_64.whl (11.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.6 MB 22.4 MB/s \n",
            "\u001b[?25hCollecting numpy==1.18.5\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 102.8 MB/s \n",
            "\u001b[?25hCollecting phonemizer==2.2.1\n",
            "  Downloading phonemizer-2.2.1-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting scipy==1.5.2\n",
            "  Downloading scipy-1.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting tensorboard==2.3.0\n",
            "  Downloading tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 26.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.13.1+cu113)\n",
            "Collecting Unidecode==1.1.1\n",
            "  Downloading Unidecode-1.1.1-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[K     |████████████████████████████████| 238 kB 78.3 MB/s \n",
            "\u001b[?25hCollecting pyworld\n",
            "  Downloading pyworld-0.3.2.tar.gz (214 kB)\n",
            "\u001b[K     |████████████████████████████████| 214 kB 75.5 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r requirements.txt (line 2)) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r requirements.txt (line 2)) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r requirements.txt (line 2)) (0.4.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r requirements.txt (line 2)) (0.56.3)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.8.0->-r requirements.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (2022.9.24)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (7.1.2)\n",
            "Collecting segments\n",
            "  Downloading segments-2.2.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.7/dist-packages (from phonemizer==2.2.1->-r requirements.txt (line 5)) (22.1.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (3.17.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (1.50.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (3.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (0.37.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.3.0->-r requirements.txt (line 7)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.3.0->-r requirements.txt (line 7)) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.3.0->-r requirements.txt (line 7)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.3.0->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.3.1->-r requirements.txt (line 3)) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard==2.3.0->-r requirements.txt (line 7)) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.3.0->-r requirements.txt (line 7)) (3.10.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa==0.8.0->-r requirements.txt (line 2)) (0.39.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.0->-r requirements.txt (line 2)) (21.3)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa==0.8.0->-r requirements.txt (line 2)) (1.4.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.3.0->-r requirements.txt (line 7)) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.3.0->-r requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.3.0->-r requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.3.0->-r requirements.txt (line 7)) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.3.0->-r requirements.txt (line 7)) (3.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.0->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa==0.8.0->-r requirements.txt (line 2)) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.8.0->-r requirements.txt (line 2)) (2.21)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segments->phonemizer==2.2.1->-r requirements.txt (line 5)) (2022.6.2)\n",
            "Collecting csvw>=1.5.6\n",
            "  Downloading csvw-3.1.3-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.7 MB/s \n",
            "\u001b[?25hCollecting clldutils>=1.7.3\n",
            "  Downloading clldutils-3.13.0-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 72.7 MB/s \n",
            "\u001b[?25hCollecting pylatexenc\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 54.9 MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from clldutils>=1.7.3->segments->phonemizer==2.2.1->-r requirements.txt (line 5)) (0.8.10)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from csvw>=1.5.6->segments->phonemizer==2.2.1->-r requirements.txt (line 5)) (4.3.3)\n",
            "Collecting rfc3986<2\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting language-tags\n",
            "  Downloading language_tags-1.1.0-py2.py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 68.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from csvw>=1.5.6->segments->phonemizer==2.2.1->-r requirements.txt (line 5)) (3.0.1)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from csvw>=1.5.6->segments->phonemizer==2.2.1->-r requirements.txt (line 5)) (2.10.3)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 660 kB/s \n",
            "\u001b[?25hCollecting rdflib\n",
            "  Downloading rdflib-6.2.0-py3-none-any.whl (500 kB)\n",
            "\u001b[K     |████████████████████████████████| 500 kB 74.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel->csvw>=1.5.6->segments->phonemizer==2.2.1->-r requirements.txt (line 5)) (2022.5)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer==2.2.1->-r requirements.txt (line 5)) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer==2.2.1->-r requirements.txt (line 5)) (5.10.0)\n",
            "Building wheels for collected packages: librosa, pyworld, pylatexenc\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.8.0-py3-none-any.whl size=201396 sha256=b5c4b8d36fc63d647ebeebfa98bb7243b0e41ac468d9367343fbdb32f1e4c2a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/1e/aa/d91797ae7e1ce11853ee100bee9d1781ae9d750e7458c95afb\n",
            "  Building wheel for pyworld (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyworld: filename=pyworld-0.3.2-cp37-cp37m-linux_x86_64.whl size=611562 sha256=1d00fcb3f14bcc16b848a96d38dbff4562e2e4a664bff7b31b2d0e301b7c1188\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/4c/9c/79f7408701787a80a2cbab87d7257fd0081fe601251abaa42b\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136835 sha256=e5d0079b28694e479ec8345f4b7ff3aa52347a13a2b86e9f57f0e27ef7d53a7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/8a/f5/33ee79d4473eb201b519fa40f989b842e373237395a3421f52\n",
            "Successfully built librosa pyworld pylatexenc\n",
            "Installing collected packages: isodate, rfc3986, rdflib, language-tags, colorama, pylatexenc, numpy, csvw, colorlog, scipy, clldutils, segments, Cython, Unidecode, tensorboard, pyworld, phonemizer, matplotlib, librosa\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 0.29.32\n",
            "    Uninstalling Cython-0.29.32:\n",
            "      Successfully uninstalled Cython-0.29.32\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.8.1\n",
            "    Uninstalling librosa-0.8.1:\n",
            "      Successfully uninstalled librosa-0.8.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n",
            "tensorflow 2.9.2 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "tensorflow 2.9.2 requires tensorboard<2.10,>=2.9, but you have tensorboard 2.3.0 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "jaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.23 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "cmdstanpy 1.0.8 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "Successfully installed Cython-0.29.21 Unidecode-1.1.1 clldutils-3.13.0 colorama-0.4.6 colorlog-6.7.0 csvw-3.1.3 isodate-0.6.1 language-tags-1.1.0 librosa-0.8.0 matplotlib-3.3.1 numpy-1.18.5 phonemizer-2.2.1 pylatexenc-2.10 pyworld-0.3.2 rdflib-6.2.0 rfc3986-1.5.0 scipy-1.5.2 segments-2.2.1 tensorboard-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  espeak-data libespeak1 libportaudio2 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak espeak-data libespeak1 libportaudio2 libsonic0\n",
            "0 upgraded, 5 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 1,219 kB of archives.\n",
            "After this operation, 3,031 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libportaudio2 amd64 19.6.0-1 [64.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsonic0 amd64 0.2.0-6 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak-data amd64 1.48.04+dfsg-5 [934 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libespeak1 amd64 1.48.04+dfsg-5 [145 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak amd64 1.48.04+dfsg-5 [61.6 kB]\n",
            "Fetched 1,219 kB in 0s (4,233 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 123942 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../libsonic0_0.2.0-6_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-6) ...\n",
            "Selecting previously unselected package espeak-data:amd64.\n",
            "Preparing to unpack .../espeak-data_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking espeak-data:amd64 (1.48.04+dfsg-5) ...\n",
            "Selecting previously unselected package libespeak1:amd64.\n",
            "Preparing to unpack .../libespeak1_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking libespeak1:amd64 (1.48.04+dfsg-5) ...\n",
            "Selecting previously unselected package espeak.\n",
            "Preparing to unpack .../espeak_1.48.04+dfsg-5_amd64.deb ...\n",
            "Unpacking espeak (1.48.04+dfsg-5) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1) ...\n",
            "Setting up espeak-data:amd64 (1.48.04+dfsg-5) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-6) ...\n",
            "Setting up libespeak1:amd64 (1.48.04+dfsg-5) ...\n",
            "Setting up espeak (1.48.04+dfsg-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "p7zip-full is already the newest version (16.02+dfsg-6).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  p7zip-rar\n",
            "0 upgraded, 1 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 43.1 kB of archives.\n",
            "After this operation, 113 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 p7zip-rar amd64 16.02-2 [43.1 kB]\n",
            "Fetched 43.1 kB in 0s (126 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package p7zip-rar.\n",
            "(Reading database ... 124267 files and directories currently installed.)\n",
            "Preparing to unpack .../p7zip-rar_16.02-2_amd64.deb ...\n",
            "Unpacking p7zip-rar (16.02-2) ...\n",
            "Setting up p7zip-rar (16.02-2) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting demjson\n",
            "  Downloading demjson-2.2.4.tar.gz (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 5.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: demjson\n",
            "  Building wheel for demjson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for demjson: filename=demjson-2.2.4-py3-none-any.whl size=73565 sha256=75fa6b627a3fb68036d8101b32359e430b3366bd0737d6553c71aea46a674230\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/94/3d/466801f4a8db8e6fce765d7a0115dfebcc55ddf6b00cd98f59\n",
            "Successfully built demjson\n",
            "Installing collected packages: demjson\n",
            "Successfully installed demjson-2.2.4\n"
          ]
        }
      ],
      "source": [
        "#@title 下载依赖库\n",
        "#@markdown 取消勾选则不会节省空间\n",
        "colab_save_space = True #@param {type:\"boolean\"}\n",
        "os.chdir('/content')\n",
        "run_command_by_line([\"git\", \"clone\", \"https://github.com/DLSeed/sovits_f0_train.git\", \"-b\", \"main\" if colab_save_space else \"main\"])\n",
        "os.chdir('/content/sovits_f0_train')\n",
        "!pip install -r requirements.txt\n",
        "!sudo apt-get install espeak -y\n",
        "!sudo apt-get install p7zip-full p7zip-rar\n",
        "!pip install demjson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZOgjdsQgKTfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43f59d39-c562-4fc0-9c23-93891f64f7a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title 加载Google云端硬盘\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N3a-FsHghwXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d764d94-6ba2-4f5a-f94f-e0286033c106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "#@title 解压数据集\n",
        "#!mkdir /content/sovits_f0_train/dataset\n",
        "%cd /content\n",
        "\n",
        "#@markdown **数据集一键制作出的压缩包，名称完全相同**\n",
        "\n",
        "#@markdown 每次解压**一个**，然后确认完成后**修改名称**、**依次**解压**所有**压缩包\n",
        "\n",
        "#@markdown 压缩包名称(不带zip)\n",
        "DATASETNAME = \"out_lamianer\"  #@param {type:\"string\"}\n",
        "#@markdown 压缩包路径(这个名字是接着上一篇的输出zip)\n",
        "ZIP_PATH = \"/content/drive/MyDrive/dataset/out_lamianer.zip\"  #@param {type:\"string\"}\n",
        "DATASETPATH = \"/content/sovits_f0_train/\" + DATASETNAME\n",
        "#%cd /content/sovits_f0_train/dataset\n",
        "!cp {ZIP_PATH} {DATASETNAME}.zip\n",
        "!unzip -q {DATASETNAME}.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 生成filelist\n",
        "import re\n",
        "import demjson\n",
        "!mkdir /content/sovits_f0_train/filelist\n",
        "%cd /content/sovits_f0_train/\n",
        "\n",
        "\n",
        "def replace_id(f_list, sp_id):\n",
        "    if re.search('wav[|](\\d+)[|]', f_list[0]):\n",
        "        split_char = \"wav|%s|\" % re.search('wav[|](\\d+)[|]', f_list[0]).group(1)\n",
        "    else:\n",
        "        split_char = \"wav|\"\n",
        "    f_list = [x.replace(split_char, f\"wav|{sp_id}|\") for x in f_list]\n",
        "    return f_list\n",
        "\n",
        "\n",
        "# data文件夹下放，各个 工程名/wavs/train(val)(这层可以不要)/xxx.wav\n",
        "# 自动读取每个工程文件夹下的train(val).txt(xxx.wav|无符号文本)\n",
        "#@markdown 默认为0\n",
        "pre = 0  #@param {type:\"string\"} \n",
        "dataset_path = \"/content\"  # 数据合集目录\n",
        "file_train_name = \"train.txt\"\n",
        "file_val_name = \"val.txt\"\n",
        "#@markdown 人物名(**必须与数据集制作时人物名相同，个数也相同，英文逗号隔开**)可以单人\n",
        "\n",
        "#@markdown 就是已经在dataset文件夹下解压出的数据文件夹\n",
        "\n",
        "#@markdown id即为此处人物排序，从0自动开始计算\n",
        "speakers = \"lamianer\" #@param {type:\"string\"}\n",
        "speakers = [speaker.strip() for speaker in speakers.split(\",\")]\n",
        "\n",
        "print(speakers)\n",
        "\n",
        "train_f = open(\"./filelist/train.txt\", \"w\", encoding=\"utf-8\")\n",
        "val_f = open(\"./filelist/val.txt\", \"w\", encoding=\"utf-8\")\n",
        "pre = int(pre)\n",
        "for i in range(pre, len(speakers) + pre):\n",
        "    with open(f\"{dataset_path}/{speakers[i]}/{file_train_name}\", \"r\", encoding=\"utf-8\") as f:\n",
        "        file_list = f.readlines()\n",
        "        train_f.writelines(replace_id(file_list, i))\n",
        "    with open(f\"{dataset_path}/{speakers[i]}/{file_val_name}\", \"r\", encoding=\"utf-8\") as f:\n",
        "        file_list = f.readlines()\n",
        "        val_f.writelines(replace_id(file_list, i))\n",
        "train_f.close()\n",
        "val_f.close()\n"
      ],
      "metadata": {
        "id": "GfkmTW1Z36iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f08bd2-eef5-4d44-9437-2b3e122fa072"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sovits_f0_train\n",
            "['lamianer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qDPyW6pnMpK9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b0P9X9SSPl0L",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03dfbee4-8563-49df-dcae-91d0f2a444ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speakers: \n",
            "\t0: lamianer\n"
          ]
        }
      ],
      "source": [
        "#@title 生成配置文件 在configs下，记得自己下载保留一份，下次用\n",
        "#@markdown 本colab内可选下载的**预模型已替换为22050hz**，id为0-7可用\n",
        "\n",
        "#@markdown **预模型请直接使用下一步下载的nyarumul.json，id0-7可用，json无需修改**\n",
        "\n",
        "#@markdown **预模型的json可以不改动，只放一个id的数据进去**\n",
        "\n",
        "# forked from https://github.com/CjangCjengh/vits/blob/main/configs/japanese_ss_base2.json\n",
        "\n",
        "#@markdown 44100采样率，训练时长要求高\n",
        "high_sample_rate = False #@param {type:\"boolean\"}\n",
        "#@markdown 配置文件名称\n",
        "json_filename = \"lamianer.json\" #@param {type:\"string\"}\n",
        "#@markdown 训练次数\n",
        "hparams_epochs = 6000 #@param {type:\"integer\"}\n",
        "#@markdown 每隔多少次step保存一次断点\n",
        "hparams_eval_interval = 2000 #@param {type:\"integer\"}\n",
        "#@markdown 单次step的文件数（建议在16以内，若使用44100采样率，在8以内）\n",
        "hparams_batch_size = 16 #@param {type:\"integer\"}\n",
        "#@markdown 训练集文件列表\n",
        "hparams_training_files = \"/content/sovits_f0_train/filelist/train.txt\" #@param {type:\"string\"}\n",
        "#@markdown 验证集文件列表\n",
        "hparams_validation_files = \"/content/sovits_f0_train/filelist/val.txt\"#@param {type:\"string\"}\n",
        "#@markdown 人物名，多个人物用英文逗号隔开，必须与上面填写的相同\n",
        "hparams_speaker = \"lamianer\" #@param {type:\"string\"}\n",
        "#@markdown 模型名\n",
        "hparams_model_name = \"nyarumul\" #@param {type:\"string\"}\n",
        "\n",
        "speakers = [speaker.strip() for speaker in hparams_speaker.split(\",\")]\n",
        "print(\"speakers: \")\n",
        "for i, speaker in enumerate(speakers):\n",
        "  print(\"\\t{a}: {b}\".format(a=i, b=speaker))\n",
        "training_json = {\n",
        "  \"train\": {\n",
        "    \"log_interval\": 200,\n",
        "    \"eval_interval\": hparams_eval_interval,\n",
        "    \"seed\": 1234 ,\n",
        "    \"epochs\": hparams_epochs,\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"betas\": [0.8, 0.99],\n",
        "    \"eps\": 1e-9,\n",
        "    \"batch_size\": hparams_batch_size,\n",
        "    \"fp16_run\": True,\n",
        "    \"lr_decay\": 0.999875,\n",
        "    \"segment_size\": 16384 if high_sample_rate else 8192,\n",
        "    \"init_lr_ratio\": 1,\n",
        "    \"warmup_epochs\": 0,\n",
        "    \"c_mel\": 45,\n",
        "    \"c_kl\": 1.0\n",
        "  },\n",
        "  \"data\": {\n",
        "    \"training_files\": hparams_training_files,\n",
        "    \"validation_files\": hparams_validation_files,\n",
        "    \"text_cleaners\":[\"english_cleaners2\"],\n",
        "    \"max_wav_value\": 32768.0,\n",
        "    \"sampling_rate\": 44100 if high_sample_rate else 22050,\n",
        "    \"filter_length\": 2048 if high_sample_rate else 1024,\n",
        "    \"hop_length\": 512 if high_sample_rate else 256,\n",
        "    \"win_length\": 2048 if high_sample_rate else 1024,\n",
        "    \"n_mel_channels\": 128 if high_sample_rate else 80,\n",
        "    \"mel_fmin\": 0.0,\n",
        "    \"mel_fmax\": None,\n",
        "    \"add_blank\": True,\n",
        "    \"n_speakers\": len(speakers) if len(speakers) > 1 else 2\n",
        "  },\n",
        "  \"model\": {\n",
        "    \"inter_channels\": 192,\n",
        "    \"hidden_channels\": 256,\n",
        "    \"filter_channels\": 768,\n",
        "    \"n_heads\": 2,\n",
        "    \"n_layers\": 6,\n",
        "    \"kernel_size\": 3,\n",
        "    \"p_dropout\": 0.1,\n",
        "    \"resblock\": \"1\",\n",
        "    \"resblock_kernel_sizes\": [3,7,11],\n",
        "    \"resblock_dilation_sizes\": [[1,3,5], [1,3,5], [1,3,5]],\n",
        "    \"upsample_rates\": [8,8,4,2] if high_sample_rate else [8,8,2,2],\n",
        "    \"upsample_initial_channel\": 512,\n",
        "    \"upsample_kernel_sizes\": [16,16,4,4],\n",
        "    \"n_layers_q\": 3,\n",
        "    \"use_spectral_norm\": False\n",
        "  },\n",
        "  \"speakers\": speakers\n",
        "}\n",
        "\n",
        "if len(speakers) > 1:\n",
        "  training_json[\"model\"][\"gin_channels\"] = 256\n",
        "\n",
        "import demjson\n",
        "os.chdir('/content/sovits_f0_train/configs')\n",
        "training_json_text = demjson.encode(training_json)\n",
        "with open(json_filename, \"w\") as file:\n",
        "  file.write(training_json_text)\n",
        "\n",
        "os.chdir('/content/sovits_f0_train')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oqSbYJ5ack09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39ffbcdf-d73c-4488-d6d4-27d36542b669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling core.pyx because it changed.\n",
            "[1/1] Cythonizing core.pyx\n",
            "/usr/local/lib/python3.7/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/sovits_f0_train/monotonic_align/core.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "running build_ext\n",
            "building 'monotonic_align.core' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I/usr/include/python3.7m -c core.c -o build/temp.linux-x86_64-3.7/core.o\n",
            "creating /content/sovits_f0_train/monotonic_align/monotonic_align\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/core.o -o /content/sovits_f0_train/monotonic_align/monotonic_align/core.cpython-37m-x86_64-linux-gnu.so\n"
          ]
        }
      ],
      "source": [
        "#@title 预处理\n",
        "os.chdir('/content/sovits_f0_train/monotonic_align')\n",
        "!python setup.py build_ext --inplace\n",
        "os.chdir('/content/sovits_f0_train')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "若**使用预模型**（2.0只能用2.0的预模型），直接在预模型基础上训练，节省时间\n",
        "\n",
        "使用预模型读取pth文件后，文件名的G_steps.pth，steps会乱是正常现象（使用v100训练，batch_size为32，colab显存不够改成了16，所以会乱掉，保存最新生成的模型即可）\n",
        "\n",
        "程序读取一次预模型后，以后断点恢复训练、用**最新生成的一组pth继续训练**"
      ],
      "metadata": {
        "id": "mXsu-rXYx2Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 22050hz预模型,id为0-7可用\n",
        "!mkdir /content/drive/MyDrive/nyarumul/\n",
        "pre_pth = True #@param {type:\"boolean\"}\n",
        "if pre_pth:\n",
        "  !wget https://huggingface.co/spaces/xiaolang/sovits_f0/resolve/main/G_50000.pth -O /content/drive/MyDrive/nyarumul/G_50000.pth\n",
        "  !wget https://huggingface.co/spaces/xiaolang/sovits_f0/resolve/main/D_50000.pth -O /content/drive/MyDrive/nyarumul/D_50000.pth"
      ],
      "metadata": {
        "id": "nz3KJCcBVTlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltU2JXpxIh-K"
      },
      "outputs": [],
      "source": [
        "#@title 训练\n",
        "\n",
        "#@markdown 启用tensorboard可视化数据，首次运行可能十分钟左右出现数据\n",
        "\n",
        "#@markdown epoch次数看/content/drive/MyDrive/数据集名/train.log\n",
        "\n",
        "#@markdown 数据集名（使用预模型则为nyarumul）\n",
        "hparams_model_name = \"nyarumul\"  # @param {type:\"string\"}\n",
        "#@markdown 模型自动保存到 /云盘路径/模型名 ,记得定期清理空间！\n",
        "param_enable_tb = True  # @param {type:\"boolean\"}\n",
        "if param_enable_tb:\n",
        "  #@markdown 云盘路径（一般不改） （定时清云盘！！！pth里保留最新的俩）\n",
        "  logdir = \"/content/drive/MyDrive/\"  # @param {type:\"string\"}\n",
        "  new_pth_dir = os.path.join(logdir, hparams_model_name)\n",
        "  get_tensorboard_showing(new_pth_dir)\n",
        "os.chdir('/content/sovits_f0_train')\n",
        "#@markdown 配置文件json名（使用预模型则为nyarumul.json）\n",
        "json_filename = \"nyarumul.json\"  # @param {type:\"string\"}\n",
        "# 这里魔改过until.py的args\n",
        "run_command_by_line([\"python\", \"train_ms.py\", \"-c\", \"configs/{json}\".format(json=json_filename), \"-m\", hparams_model_name, \"-l\",logdir])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 精简模型\n",
        "import torch\n",
        "#@markdown 模型名，数字是最新的模型steps\n",
        "new_model_name = \"G_30000.pth\"  # @param {type:\"string\"}\n",
        "checkpoint_dict = torch.load(f\"{new_pth_dir}/{new_model_name}\")\n",
        "iteration = checkpoint_dict['iteration']\n",
        "learning_rate = checkpoint_dict['learning_rate']\n",
        "optimizer = checkpoint_dict['optimizer']\n",
        "saved_state_dict = checkpoint_dict['model']\n",
        "print(iteration)\n",
        "#@markdown 输出xxx_epoch.pth\n",
        "torch.save({'model': saved_state_dict,\n",
        "  'iteration': None,\n",
        "  'optimizer': None,\n",
        "  'learning_rate': None}, f'{new_pth_dir}/{iteration}_epochs.pth')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lwa9GQx1X0L_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb93c98f-9330-4bfd-d0c8-b359b30b1ea5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "153\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}